{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analysis of Perturbations in Single-Cell RNA-Seq Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a `PertData` object to load and handle perturbation data.\n",
    "We specify that we want to load the `norman` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pertdata as pt\n",
    "\n",
    "norman = pt.PertData.from_repo(name=\"norman\", save_dir=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Preparation of Single-Cell RNA-Seq Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual perturbation data is stored in an [`AnnData`](https://anndata.readthedocs.io/en/latest/) object.\n",
    "`AnnData` is specifically designed for matrix-like data.\n",
    "By this we mean that we have $n$ observations, each of which can be represented as $d$-dimensional vectors, where each dimension corresponds to a variable or feature.\n",
    "Both the rows and columns of this $n \\times d$-matrix are special in the sense that they are indexed.\n",
    "For instance, in scRNA-seq data, each row corresponds to a cell with a barcode, and each column corresponds to a gene with a gene identifier.\n",
    "\n",
    "Here we show how to access the **gene expression matrix**, the **perturbations labels**, the **control labels**, and the **gene names**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(norman)\n",
    "print(norman.adata)\n",
    "\n",
    "X = norman.adata.X\n",
    "y_pert = norman.adata.obs[\"condition\"]\n",
    "y_ctrl = norman.adata.obs[\"control\"]\n",
    "gene_names = norman.adata.var[\"gene_name\"]\n",
    "\n",
    "print(f\"X.shape={X.shape}\")  # type: ignore\n",
    "print(f\"y_pert.shape={y_pert.shape}\")\n",
    "print(f\"y_ctrl.shape={y_ctrl.shape}\")\n",
    "print(f\"gene_names.shape={gene_names.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, in a perturbation dataset, we find $k$ cell lines.\n",
    "Usually, one cell line remains unperturbed, and the others are cultivated separately (with different perturbations, i.e., gene knockouts).\n",
    "The mRNA of usually a few thousand cells of each cell line is sequenced (using a single-cell RNA sequencing protocol), generating the $n$ $d$-dimensional gene expression profiles.\n",
    "In particular, perturbation labels are available (i.e., we know which genes were knocked out in each cell line).\n",
    "\n",
    "Before we can take a closer look at our data, we need to fix the perturbation labels, because they might be expressed ambiguously (e.g., single-gene perturbations can be expressed as `ctrl+<gene1>` or `<gene1>+ctrl`, falsely leading to two distinct labels for the perturbation of `<gene1>`).\n",
    "\n",
    "The `PertData` object already contains the fixed perturbation labels, which were computed during initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique perturbations (unfixed): {len(set(norman.adata.obs['condition']))}\")\n",
    "print(f\"Unique perturbations (fixed): {len(set(norman.adata.obs['condition_fixed']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we will work with single-gene perturbations only.\n",
    "Hence, we have to filter out double-gene perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Filtering out double-gene perturbations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following code to filter out double-gene perturbations from the `AnnData` object.\n",
    "Double-gene perturbations can be identified by a `+` in the fixed perturbation labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_mask = ~norman.adata.obs[\"condition_fixed\"].str.contains(r\"\\+\")\n",
    "indexes_to_keep = filter_mask[filter_mask].index\n",
    "adata_single = norman.adata[indexes_to_keep].copy()  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adata_single)\n",
    "print(f\"Unique perturbations: {len(set(adata_single.obs['condition_fixed']))}\")\n",
    "print(\"Number of samples per condition:\")\n",
    "print(adata_single.obs[\"condition_fixed\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because gene expression data is very sparse, i.e., the expression is often not measured successfully or correctly, we will limit our experiment to the 128 genes with the highest variances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Selecting high-variance genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following code.\n",
    "Make a new `AnnData` object that only contains the top $d$ genes with the highest variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of top genes to select.\n",
    "d = 128\n",
    "\n",
    "# Compute the gene variances.\n",
    "gene_variances = adata_single.X.toarray().var(axis=0)  # type: ignore\n",
    "\n",
    "# Sort the gene variances in descending order and get the indexes of the top d genes.\n",
    "sorted_indexes = gene_variances.argsort()[::-1]\n",
    "\n",
    "# Get the indexes of the top d genes.\n",
    "top_gene_indexes = sorted_indexes[:d]\n",
    "\n",
    "# Get the gene names of the top d genes.\n",
    "top_genes = adata_single.var[\"gene_name\"].iloc[top_gene_indexes]\n",
    "\n",
    "# Get the variances of the top d genes.\n",
    "top_variances = gene_variances[top_gene_indexes]\n",
    "\n",
    "# Print the top d genes with the highest variances.\n",
    "print(f\"Top {d} genes with highest variances:\")\n",
    "for gene, variance in zip(top_genes, top_variances):\n",
    "    print(f\"{gene:15}: {variance:.2f}\")\n",
    "\n",
    "# Create a new AnnData object with only the top d genes.\n",
    "adata_single_top_genes = adata_single[:, top_gene_indexes].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an autoencoder trained on RNA-Seq data can offer several significant advantages, especially when dealing with the complexities inherent to gene expression datasets.\n",
    "\n",
    "Here, we will highlight two use cases.\n",
    "\n",
    "1. High Dimensionality of RNA-Seq Data\n",
    "    - Challenge: RNA-Seq datasets often comprise expression levels for thousands of genes (features), which can lead to the \"curse of dimensionality.\" High-dimensional data can make models like Multi-Layer Perceptrons (MLPs) computationally intensive and prone to overfitting.\n",
    "    - Solution: Autoencoders can learn a compressed, lower-dimensional representation (latent space) of the data by encoding the input features into a smaller set of latent variables. This reduced representation retains the most critical information, making it easier and more efficient for downstream models (like an MLP) to process the data.\n",
    "\n",
    "2. Capturing Complex Patterns\n",
    "    - Challenge: Gene expression data may contain intricate, nonlinear relationships that are not easily captured by traditional linear dimensionality reduction techniques like PCA.\n",
    "    - Solution: Autoencoders, especially deep or variational ones, can capture complex, nonlinear relationships within the data. The learned latent representations can encapsulate meaningful biological patterns and interactions among genes, providing richer features for classification tasks.\n",
    "\n",
    "Further reading:\n",
    "\n",
    "Carlos Ruiz-Arenas, Irene Marín-Goñi, Liewei Wang, Idoia Ochoa, Luis A Pérez-Jurado, Mikel Hernaez, NetActivity enhances transcriptional signals by combining gene expression into robust gene set activity scores through interpretable autoencoders, Nucleic Acids Research, Volume 52, Issue 9, 22 May 2024, Page e44, https://doi.org/10.1093/nar/gkae197\n",
    "\n",
    "Abstract:\n",
    "\n",
    "\"Grouping gene expression into gene set activity scores (GSAS) provides better biological insights than studying individual genes. However, existing gene set projection methods cannot return representative, robust, and interpretable GSAS. We developed NetActivity, a machine learning framework that generates GSAS based on a sparsely-connected autoencoder, where each neuron in the inner layer represents a gene set. [...]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Autoencoder on Single-Cell RNA-Seq Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we prepare our data.\n",
    "\n",
    "Note that we shuffle the data in the `train_loader` but not in the `test_loader`.\n",
    "\n",
    "Shuffling the training data is a common practice to ensure that the model does not learn the order of the data.\n",
    "It helps in breaking correlations by preventing the model from learning any unintended patterns or correlations that might exist in the order of the training data.\n",
    "\n",
    "Shuffling is typically not used for the testing data because non-shuffled data ensures that the evaluation is consistent and reproducible.\n",
    "The model is tested on the same data in the same order each time, which is important for comparing performance across different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Convert the gene expression matrix to a PyTorch tensor.\n",
    "X = torch.tensor(data=adata_single_top_genes.X.toarray(), dtype=torch.float32)  # type: ignore\n",
    "\n",
    "# Create a PyTorch dataset.\n",
    "dataset = TensorDataset(X, X)\n",
    "\n",
    "# Create train and test datasets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(\n",
    "    dataset=dataset, lengths=[train_size, test_size]\n",
    ")\n",
    "\n",
    "# Number of workers.\n",
    "num_workers = 3\n",
    "\n",
    "# Create train and test data loaders.\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Creating an autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an autoencoder for the RNA-Seq data using the `Autoencoder` class from `models.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from models import Autoencoder\n",
    "\n",
    "# Get the number of features.\n",
    "n_features = X.shape[1]\n",
    "print(f\"n_features={n_features}\")\n",
    "\n",
    "# Get the number of samples.\n",
    "n_samples = X.shape[0]  # = len(train_dataset) + len(test_dataset) = len(dataset)\n",
    "print(f\"n_samples={n_samples}\")\n",
    "\n",
    "# Create the autoencoder\n",
    "autoencoder = Autoencoder(in_features=n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train the autoencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "# Initialize the CSV logger.\n",
    "logger = CSVLogger(save_dir=\"lightning_logs\", name=\"ae_experiment\")\n",
    "\n",
    "# Train the autoencoder.\n",
    "trainer = pl.Trainer(max_epochs=4, logger=logger)\n",
    "trainer.fit(model=autoencoder, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we test the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the autoencoder.\n",
    "trainer.test(model=autoencoder, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Plotting the training loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following code to plot the training loss over the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def plot_train_loss(logfile: str) -> None:\n",
    "    \"\"\"Plot the training loss from a PyTorch Lightning log file.\"\"\"\n",
    "    print(f\"logfile={logfile}\")\n",
    "    log = pd.read_csv(filepath_or_buffer=logfile)\n",
    "    plt.plot(log[\"step\"], log[\"train_loss\"])\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Train Loss\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Construct the path to the most recent version directory.\n",
    "most_recent_metrics_file = os.path.join(\n",
    "    logger.save_dir, logger.name, f\"version_{logger.version}\", \"metrics.csv\"\n",
    ")\n",
    "\n",
    "# Plot the training loss.\n",
    "plot_train_loss(logfile=most_recent_metrics_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
